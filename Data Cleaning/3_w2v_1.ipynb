{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3_w2v_1\n",
    "    # generate Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 35890 labeled train reviews, \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rl/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (20,21,22,23,24,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from files \n",
    "train = pd.read_csv(\"7282_1.csv\", header=0, encoding='utf8')\n",
    "train = train[pd.notnull(train[\"reviews.text\"])]\n",
    "train=train.reset_index(drop=True)\n",
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print (\"Read %d labeled train reviews, \\n\" % (train[\"reviews.text\"].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rl/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /home/rl/anaconda2/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/home/rl/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:219: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/rl/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://ihg.co/V4S29\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/rl/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://ihg.co/V4S20\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/rl/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:219: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/rl/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"https://lyft.com/i/krystie877424\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/rl/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.daybreakhotels.com/it-it/italia/trezzano-sul-naviglio/best-western-hotel-goldenmile\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/rl/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"https://goo.gl/hLBGHs\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/rl/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"https://goo.gl/6iJX3b\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/rl/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"https://goo.gl/6g9cff\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/rl/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://travel.uk.reuters.com/locations/new-york/hotels/new-york-marriott-marquis\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/rl/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.beachcove520.com/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/rl/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://ow.ly/qnfmt\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/rl/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://ow.ly/qjodh\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in train[\"reviews.text\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138241\n",
      "[u'pleasant', u'min', u'walk', u'along', u'the', u'sea', u'front', u'to', u'the', u'water', u'bus']\n"
     ]
    }
   ],
   "source": [
    " # Check how many sentences we have in total - should be around 850,000+\n",
    "print len(sentences)\n",
    "print sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-09 14:53:39,604 : INFO : collecting all words and their counts\n",
      "2017-11-09 14:53:39,607 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-09 14:53:39,681 : INFO : PROGRESS: at sentence #10000, processed 120170 words, keeping 7986 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-09 14:53:39,821 : INFO : PROGRESS: at sentence #20000, processed 241786 words, keeping 10826 word types\n",
      "2017-11-09 14:53:39,997 : INFO : PROGRESS: at sentence #30000, processed 364821 words, keeping 13374 word types\n",
      "2017-11-09 14:53:40,120 : INFO : PROGRESS: at sentence #40000, processed 488391 words, keeping 16738 word types\n",
      "2017-11-09 14:53:40,234 : INFO : PROGRESS: at sentence #50000, processed 609847 words, keeping 18324 word types\n",
      "2017-11-09 14:53:40,410 : INFO : PROGRESS: at sentence #60000, processed 733794 words, keeping 20429 word types\n",
      "2017-11-09 14:53:40,689 : INFO : PROGRESS: at sentence #70000, processed 854583 words, keeping 21889 word types\n",
      "2017-11-09 14:53:40,815 : INFO : PROGRESS: at sentence #80000, processed 974476 words, keeping 23298 word types\n",
      "2017-11-09 14:53:40,935 : INFO : PROGRESS: at sentence #90000, processed 1092967 words, keeping 24653 word types\n",
      "2017-11-09 14:53:41,042 : INFO : PROGRESS: at sentence #100000, processed 1210201 words, keeping 25804 word types\n",
      "2017-11-09 14:53:41,136 : INFO : PROGRESS: at sentence #110000, processed 1335597 words, keeping 27659 word types\n",
      "2017-11-09 14:53:41,217 : INFO : PROGRESS: at sentence #120000, processed 1453363 words, keeping 28612 word types\n",
      "2017-11-09 14:53:41,292 : INFO : PROGRESS: at sentence #130000, processed 1569276 words, keeping 30000 word types\n",
      "2017-11-09 14:53:41,350 : INFO : collected 30944 word types from a corpus of 1666791 raw words and 138241 sentences\n",
      "2017-11-09 14:53:41,368 : INFO : Loading a fresh vocabulary\n",
      "2017-11-09 14:53:41,443 : INFO : min_count=120 retains 1244 unique words (4% of original 30944, drops 29700)\n",
      "2017-11-09 14:53:41,453 : INFO : min_count=120 leaves 1466523 word corpus (87% of original 1666791, drops 200268)\n",
      "2017-11-09 14:53:41,463 : INFO : deleting the raw counts dictionary of 30944 items\n",
      "2017-11-09 14:53:41,477 : INFO : sample=0.001 downsamples 61 most-common words\n",
      "2017-11-09 14:53:41,484 : INFO : downsampling leaves estimated 962184 word corpus (65.6% of prior 1466523)\n",
      "2017-11-09 14:53:41,490 : INFO : estimated required memory for 1244 words and 100 dimensions: 1617200 bytes\n",
      "2017-11-09 14:53:41,511 : INFO : resetting layer weights\n",
      "2017-11-09 14:53:41,559 : INFO : training model with 4 workers on 1244 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-09 14:53:42,600 : INFO : PROGRESS: at 5.10% examples, 246293 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 14:53:43,664 : INFO : PROGRESS: at 10.41% examples, 244676 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 14:53:44,686 : INFO : PROGRESS: at 16.44% examples, 257076 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 14:53:45,735 : INFO : PROGRESS: at 24.30% examples, 282510 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-09 14:53:46,748 : INFO : PROGRESS: at 29.85% examples, 279476 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 14:53:47,754 : INFO : PROGRESS: at 38.19% examples, 298656 words/s, in_qsize 8, out_qsize 1\n",
      "2017-11-09 14:53:48,763 : INFO : PROGRESS: at 46.19% examples, 310289 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 14:53:49,772 : INFO : PROGRESS: at 50.11% examples, 295301 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 14:53:50,811 : INFO : PROGRESS: at 57.34% examples, 299777 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-09 14:53:51,827 : INFO : PROGRESS: at 64.68% examples, 304365 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 14:53:52,831 : INFO : PROGRESS: at 73.16% examples, 313551 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 14:53:53,836 : INFO : PROGRESS: at 81.88% examples, 321722 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 14:53:54,873 : INFO : PROGRESS: at 88.12% examples, 319513 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-09 14:53:55,897 : INFO : PROGRESS: at 96.14% examples, 323719 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 14:53:56,315 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-11-09 14:53:56,323 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-09 14:53:56,329 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-09 14:53:56,331 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-09 14:53:56,332 : INFO : training on 8333955 raw words (4810502 effective words) took 14.7s, 326436 effective words/s\n",
      "2017-11-09 14:53:56,336 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-11-09 14:53:56,366 : INFO : saving Word2Vec object under 100features_120winwords_10context, separately None\n",
      "2017-11-09 14:53:56,381 : INFO : not storing attribute syn0norm\n",
      "2017-11-09 14:53:56,384 : INFO : not storing attribute cum_table\n",
      "2017-11-09 14:53:56,430 : INFO : saved 100features_120winwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 100    # Word vector dimensionality                      \n",
    "min_word_count = 120  # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'gentleman', 0.8386572003364563),\n",
       " (u'lady', 0.8232420682907104),\n",
       " (u'woman', 0.8022923469543457),\n",
       " (u'girl', 0.7899739742279053),\n",
       " (u'guy', 0.764091968536377),\n",
       " (u'receptionist', 0.7487019300460815),\n",
       " (u'attendant', 0.7397534251213074),\n",
       " (u'owner', 0.7303192615509033),\n",
       " (u'clerk', 0.7267169952392578),\n",
       " (u'employee', 0.7065222859382629)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'proximity', 0.675235390663147),\n",
       " (u'neighborhood', 0.6327090263366699),\n",
       " (u'downtown', 0.6237961053848267),\n",
       " (u'located', 0.617552638053894),\n",
       " (u'value', 0.607701301574707),\n",
       " (u'venue', 0.5929969549179077),\n",
       " (u'town', 0.5890241861343384),\n",
       " (u'interstate', 0.5826400518417358),\n",
       " (u'spot', 0.5715011954307556),\n",
       " (u'convenience', 0.5645418763160706)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'rate', 0.7875528335571289),\n",
       " (u'cost', 0.6930409669876099),\n",
       " (u'value', 0.6706050038337708),\n",
       " (u'pricing', 0.6561794877052307),\n",
       " (u'prices', 0.5955599546432495),\n",
       " (u'rates', 0.5929273366928101),\n",
       " (u'money', 0.5635597705841064),\n",
       " (u'priced', 0.5359944105148315),\n",
       " (u'bargain', 0.49203598499298096),\n",
       " (u'overall', 0.48928511142730713)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'hospitality', 0.4894063472747803),\n",
       " (u'driver', 0.4685381054878235),\n",
       " (u'concierge', 0.4596795439720154),\n",
       " (u'staff', 0.45761263370513916),\n",
       " (u'pricing', 0.43609046936035156),\n",
       " (u'food', 0.43171125650405884),\n",
       " (u'services', 0.4260099530220032),\n",
       " (u'crew', 0.41115158796310425),\n",
       " (u'chef', 0.40882542729377747),\n",
       " (u'value', 0.39425352215766907)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'terrible', 0.7821382284164429),\n",
       " (u'horrible', 0.7735625505447388),\n",
       " (u'okay', 0.7328379154205322),\n",
       " (u'funny', 0.6945334672927856),\n",
       " (u'disgusting', 0.6840559840202332),\n",
       " (u'unpleasant', 0.6731683015823364),\n",
       " (u'acceptable', 0.6700491905212402),\n",
       " (u'odd', 0.6647114753723145),\n",
       " (u'gross', 0.6554749011993408),\n",
       " (u'poor', 0.6524474024772644)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
